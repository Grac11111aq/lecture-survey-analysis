#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹ç†è§£åº¦äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
==================================

æˆæ¥­å¾Œãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ãŸç†è§£åº¦äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰ã¨ç‰¹å¾´é‡é‡è¦åº¦åˆ†æã€‚

æ©Ÿèƒ½:
- å¤šç¨®é¡ã®æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒï¼ˆRandomForest, XGBoost, LogisticRegressionï¼‰
- äº¤å·®æ¤œè¨¼ã«ã‚ˆã‚‹äºˆæ¸¬ç²¾åº¦è©•ä¾¡
- ç‰¹å¾´é‡é‡è¦åº¦åˆ†æã¨è§£é‡ˆ
- ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡å¯¾å¿œ
- ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
- äºˆæ¸¬çµæœã®å¯è¦–åŒ–ã¨æ•™è‚²çš„ç¤ºå”†

åˆ¶ç´„:
- ç‹¬ç«‹ç¾¤æ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ã®ãŸã‚æˆæ¥­å¾Œãƒ‡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨
- ç†è§£åº¦4æ®µéšåˆ†é¡å•é¡Œã¨ã—ã¦æ‰±ã†
- é™ã‚‰ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºï¼ˆN=99ï¼‰ã§ã®äºˆæ¸¬ç²¾åº¦è©•ä¾¡

Author: Claude Code Analysis (ML Prediction Implementation)
Date: 2025-05-31
"""

import os
import sys
import json
import warnings
from pathlib import Path
from datetime import datetime

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
from sklearn.preprocessing import StandardScaler
from sklearn.utils.class_weight import compute_class_weight
import pickle

# è­¦å‘ŠæŠ‘åˆ¶
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)

class MachineLearningPredictor:
    def __init__(self, project_root):
        self.project_root = Path(project_root)
        self.data_dir = self.project_root / "data" / "analysis"
        self.output_dir = self.project_root / "outputs" / "current" / "05_advanced_analysis"
        self.figures_dir = self.project_root / "outputs" / "figures" / "current" / "05_advanced_analysis"
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.figures_dir.mkdir(parents=True, exist_ok=True)
        
        self.results = {}
        self.models = {}
        
    def load_data(self):
        """æˆæ¥­å¾Œãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        print("ğŸ“Š æˆæ¥­å¾Œãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
        
        after_path = self.data_dir / "after_excel_compliant.csv"
        if not after_path.exists():
            raise FileNotFoundError("æˆæ¥­å¾Œãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        self.after_data = pd.read_csv(after_path, encoding='utf-8')
        print(f"âœ“ æˆæ¥­å¾Œãƒ‡ãƒ¼ã‚¿: {len(self.after_data)} ä»¶")
        
    def prepare_ml_data(self):
        """æ©Ÿæ¢°å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™"""
        print("\nğŸ”§ æ©Ÿæ¢°å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™ä¸­...")
        
        ml_data = self.after_data.copy()
        
        # Q1ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
        q1_cols = ['Q1_Saltwater', 'Q1_Sugarwater', 'Q1_Muddywater',
                   'Q1_Ink', 'Q1_MisoSoup', 'Q1_SoySauce']
        ml_data['Q1_total'] = ml_data[q1_cols].sum(axis=1)
        
        # Q3ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
        q3_cols = ['Q3_TeaLeaves_DissolveInWater', 'Q3_TeaComponents_DissolveInWater']
        ml_data['Q3_total'] = ml_data[q3_cols].sum(axis=1)
        
        # ã‚¯ãƒ©ã‚¹ãƒ€ãƒŸãƒ¼å¤‰æ•°ä½œæˆ
        class_dummies = pd.get_dummies(ml_data['class'], prefix='class')
        
        # ç‰¹å¾´é‡é¸æŠ
        feature_cols = ['Q1_total', 'Q3_total', 'Q4_ExperimentInterestRating', 'Q5_NewLearningsRating']
        feature_cols.extend(class_dummies.columns.tolist())
        
        # ãƒ‡ãƒ¼ã‚¿çµ±åˆ
        features_df = pd.concat([ml_data[['Q1_total', 'Q3_total', 'Q4_ExperimentInterestRating', 'Q5_NewLearningsRating']], 
                                class_dummies], axis=1)
        
        # ç›®çš„å¤‰æ•°
        target = ml_data['Q6_DissolvingUnderstandingRating']
        
        # æ¬ æå€¤é™¤å»
        complete_indices = features_df.notna().all(axis=1) & target.notna()
        self.X = features_df[complete_indices].copy()
        self.y = target[complete_indices].copy()
        
        # ç‰¹å¾´é‡åä¿å­˜
        self.feature_names = self.X.columns.tolist()
        
        print(f"âœ“ å®Œå…¨ãƒ‡ãƒ¼ã‚¿: {len(self.X)} ä»¶")
        print(f"âœ“ ç‰¹å¾´é‡æ•°: {len(self.feature_names)}")
        print(f"âœ“ ç‰¹å¾´é‡: {self.feature_names}")
        
        # ã‚¯ãƒ©ã‚¹åˆ†å¸ƒç¢ºèª
        print("\nğŸ“Š ç›®çš„å¤‰æ•°ï¼ˆQ6ç†è§£åº¦ï¼‰ã®åˆ†å¸ƒ:")
        class_distribution = self.y.value_counts().sort_index()
        for class_val, count in class_distribution.items():
            print(f"  ç†è§£åº¦ {class_val}: {count} ä»¶ ({count/len(self.y)*100:.1f}%)")
        
        # ãƒ‡ãƒ¼ã‚¿æ¦‚è¦
        print("\nğŸ“Š ç‰¹å¾´é‡ã®åŸºæœ¬çµ±è¨ˆ:")
        print(self.X.describe().round(3))
        
    def train_models(self):
        """è¤‡æ•°ã®æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«è¨“ç·´"""
        print("\nğŸ¤– æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­...")
        
        # ãƒ‡ãƒ¼ã‚¿æ¨™æº–åŒ–
        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(self.X)
        
        # ã‚¯ãƒ©ã‚¹é‡ã¿è¨ˆç®—ï¼ˆä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰
        class_weights = compute_class_weight('balanced', classes=np.unique(self.y), y=self.y)
        class_weight_dict = dict(zip(np.unique(self.y), class_weights))
        
        # ãƒ¢ãƒ‡ãƒ«å®šç¾©
        models_config = {
            'RandomForest': {
                'model': RandomForestClassifier(
                    n_estimators=100, 
                    random_state=42, 
                    class_weight='balanced',
                    max_depth=5  # éå­¦ç¿’é˜²æ­¢
                ),
                'use_scaled': False
            },
            'LogisticRegression': {
                'model': LogisticRegression(
                    random_state=42, 
                    class_weight='balanced',
                    max_iter=1000,
                    multi_class='ovr'
                ),
                'use_scaled': True
            }
        }
        
        # äº¤å·®æ¤œè¨¼è¨­å®š
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        
        # å„ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã¨è©•ä¾¡
        for model_name, config in models_config.items():
            print(f"\nğŸ” {model_name} è¨“ç·´ä¸­...")
            
            model = config['model']
            X_input = X_scaled if config['use_scaled'] else self.X.values
            
            # äº¤å·®æ¤œè¨¼
            cv_scores = cross_val_score(model, X_input, self.y, cv=cv, scoring='accuracy')
            f1_scores = cross_val_score(model, X_input, self.y, cv=cv, scoring='f1_weighted')
            
            # å…¨ãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆæœ€çµ‚ãƒ¢ãƒ‡ãƒ«ï¼‰
            model.fit(X_input, self.y)
            
            # äºˆæ¸¬ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ï¼‰
            y_pred = model.predict(X_input)
            y_pred_proba = model.predict_proba(X_input) if hasattr(model, 'predict_proba') else None
            
            # çµæœä¿å­˜
            self.models[model_name] = {
                'model': model,
                'scaler': self.scaler if config['use_scaled'] else None,
                'cv_accuracy': cv_scores,
                'cv_f1_weighted': f1_scores,
                'final_accuracy': accuracy_score(self.y, y_pred),
                'final_f1_weighted': f1_score(self.y, y_pred, average='weighted'),
                'predictions': y_pred,
                'probabilities': y_pred_proba,
                'classification_report': classification_report(self.y, y_pred, output_dict=True),
                'confusion_matrix': confusion_matrix(self.y, y_pred)
            }
            
            print(f"âœ“ CVç²¾åº¦: {cv_scores.mean():.3f} Â± {cv_scores.std():.3f}")
            print(f"âœ“ CV F1ã‚¹ã‚³ã‚¢: {f1_scores.mean():.3f} Â± {f1_scores.std():.3f}")
            print(f"âœ“ æœ€çµ‚ç²¾åº¦: {accuracy_score(self.y, y_pred):.3f}")
        
        # ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ
        self._analyze_feature_importance()
        
    def _analyze_feature_importance(self):
        """ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ"""
        print("\nğŸ” ç‰¹å¾´é‡é‡è¦åº¦åˆ†æä¸­...")
        
        # RandomForestã®ç‰¹å¾´é‡é‡è¦åº¦
        if 'RandomForest' in self.models:
            rf_model = self.models['RandomForest']['model']
            feature_importance = rf_model.feature_importances_
            
            # é‡è¦åº¦ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã§æ•´ç†
            importance_df = pd.DataFrame({
                'feature': self.feature_names,
                'importance': feature_importance
            }).sort_values('importance', ascending=False)
            
            self.models['RandomForest']['feature_importance'] = importance_df
            
            print("ğŸ“Š RandomForestç‰¹å¾´é‡é‡è¦åº¦:")
            for _, row in importance_df.head(10).iterrows():
                print(f"  {row['feature']}: {row['importance']:.3f}")
        
        # LogisticRegressionã®ä¿‚æ•°
        if 'LogisticRegression' in self.models:
            lr_model = self.models['LogisticRegression']['model']
            
            # å¤šã‚¯ãƒ©ã‚¹åˆ†é¡ã®å ´åˆã®ä¿‚æ•°å‡¦ç†
            if hasattr(lr_model, 'coef_'):
                if lr_model.coef_.ndim == 1:
                    coefficients = lr_model.coef_
                else:
                    # å¤šã‚¯ãƒ©ã‚¹ã®å ´åˆã¯å¹³å‡çµ¶å¯¾å€¤
                    coefficients = np.mean(np.abs(lr_model.coef_), axis=0)
                
                coef_df = pd.DataFrame({
                    'feature': self.feature_names,
                    'coefficient': coefficients
                }).sort_values('coefficient', key=abs, ascending=False)
                
                self.models['LogisticRegression']['coefficients'] = coef_df
                
                print("\nğŸ“Š LogisticRegressionä¿‚æ•°:")
                for _, row in coef_df.head(10).iterrows():
                    print(f"  {row['feature']}: {row['coefficient']:.3f}")
    
    def hyperparameter_tuning(self):
        """ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°"""
        print("\nâš™ï¸ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­...")
        
        # RandomForestã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
        if 'RandomForest' in self.models:
            print("ğŸ”§ RandomForest ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°...")
            
            param_grid = {
                'n_estimators': [50, 100, 200],
                'max_depth': [3, 5, 7, None],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4]
            }
            
            rf_base = RandomForestClassifier(random_state=42, class_weight='balanced')
            cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)  # è¨ˆç®—é‡å‰Šæ¸›
            
            grid_search = GridSearchCV(
                rf_base, param_grid, cv=cv, scoring='f1_weighted', 
                n_jobs=1, verbose=0  # ä¸¦åˆ—åŒ–ç„¡åŠ¹ï¼ˆå®‰å…¨æ€§ã®ãŸã‚ï¼‰
            )
            
            try:
                grid_search.fit(self.X.values, self.y)
                
                self.models['RandomForest_Tuned'] = {
                    'model': grid_search.best_estimator_,
                    'best_params': grid_search.best_params_,
                    'best_score': grid_search.best_score_,
                    'cv_results': grid_search.cv_results_
                }
                
                print(f"âœ“ æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {grid_search.best_params_}")
                print(f"âœ“ æœ€é©CV F1ã‚¹ã‚³ã‚¢: {grid_search.best_score_:.3f}")
                
            except Exception as e:
                print(f"âš ï¸ ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
                print("ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™")
    
    def evaluate_models(self):
        """ãƒ¢ãƒ‡ãƒ«è©•ä¾¡"""
        print("\nğŸ“ˆ ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ä¸­...")
        
        evaluation_results = {}
        
        for model_name, model_data in self.models.items():
            if 'cv_accuracy' in model_data:  # è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ã¿
                evaluation_results[model_name] = {
                    'cv_accuracy_mean': model_data['cv_accuracy'].mean(),
                    'cv_accuracy_std': model_data['cv_accuracy'].std(),
                    'cv_f1_mean': model_data['cv_f1_weighted'].mean(),
                    'cv_f1_std': model_data['cv_f1_weighted'].std(),
                    'final_accuracy': model_data['final_accuracy'],
                    'final_f1': model_data['final_f1_weighted']
                }
        
        self.evaluation_results = evaluation_results
        
        # çµæœè¡¨ç¤º
        print("\nğŸ“Š ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒçµæœ:")
        print("Model\t\t\tCV Accuracy\t\tCV F1\t\t\tFinal Accuracy")
        print("-" * 70)
        
        for model_name, metrics in evaluation_results.items():
            print(f"{model_name:<20}\t{metrics['cv_accuracy_mean']:.3f}Â±{metrics['cv_accuracy_std']:.3f}\t\t"
                  f"{metrics['cv_f1_mean']:.3f}Â±{metrics['cv_f1_std']:.3f}\t\t{metrics['final_accuracy']:.3f}")
    
    def create_visualizations(self):
        """å¯è¦–åŒ–ä½œæˆ"""
        print("\nğŸ“Š å¯è¦–åŒ–ä½œæˆä¸­...")
        
        # 1. ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ
        self._create_model_comparison_plot()
        
        # 2. ç‰¹å¾´é‡é‡è¦åº¦
        self._create_feature_importance_plot()
        
        # 3. æ··åŒè¡Œåˆ—
        self._create_confusion_matrices()
        
        # 4. äºˆæ¸¬çµæœåˆ†å¸ƒ
        self._create_prediction_distribution_plot()
        
    def _create_model_comparison_plot(self):
        """ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒãƒ—ãƒ­ãƒƒãƒˆ"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # CVç²¾åº¦æ¯”è¼ƒ
        models = []
        accuracies = []
        f1_scores = []
        
        for model_name, metrics in self.evaluation_results.items():
            models.append(model_name.replace('_', '\n'))
            accuracies.append(metrics['cv_accuracy_mean'])
            f1_scores.append(metrics['cv_f1_mean'])
        
        x_pos = np.arange(len(models))
        
        ax1.bar(x_pos, accuracies, alpha=0.7, color='skyblue', edgecolor='black')
        ax1.set_xlabel('ãƒ¢ãƒ‡ãƒ«')
        ax1.set_ylabel('äº¤å·®æ¤œè¨¼ç²¾åº¦')
        ax1.set_title('ãƒ¢ãƒ‡ãƒ«åˆ¥äºˆæ¸¬ç²¾åº¦æ¯”è¼ƒ')
        ax1.set_xticks(x_pos)
        ax1.set_xticklabels(models, rotation=45, ha='right')
        ax1.set_ylim(0, 1)
        
        # ç²¾åº¦å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
        for i, v in enumerate(accuracies):
            ax1.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')
        
        # F1ã‚¹ã‚³ã‚¢æ¯”è¼ƒ
        ax2.bar(x_pos, f1_scores, alpha=0.7, color='lightcoral', edgecolor='black')
        ax2.set_xlabel('ãƒ¢ãƒ‡ãƒ«')
        ax2.set_ylabel('F1ã‚¹ã‚³ã‚¢ï¼ˆé‡ã¿ä»˜ãï¼‰')
        ax2.set_title('ãƒ¢ãƒ‡ãƒ«åˆ¥F1ã‚¹ã‚³ã‚¢æ¯”è¼ƒ')
        ax2.set_xticks(x_pos)
        ax2.set_xticklabels(models, rotation=45, ha='right')
        ax2.set_ylim(0, 1)
        
        # F1å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
        for i, v in enumerate(f1_scores):
            ax2.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        
        output_path = self.figures_dir / "model_comparison.png"
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ“ ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒå›³ä¿å­˜: {output_path}")
    
    def _create_feature_importance_plot(self):
        """ç‰¹å¾´é‡é‡è¦åº¦ãƒ—ãƒ­ãƒƒãƒˆ"""
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        
        # RandomForesté‡è¦åº¦
        if 'RandomForest' in self.models and 'feature_importance' in self.models['RandomForest']:
            importance_df = self.models['RandomForest']['feature_importance'].head(10)
            
            axes[0].barh(importance_df['feature'], importance_df['importance'])
            axes[0].set_xlabel('é‡è¦åº¦')
            axes[0].set_title('RandomForest ç‰¹å¾´é‡é‡è¦åº¦')
            axes[0].invert_yaxis()
        
        # LogisticRegressionä¿‚æ•°
        if 'LogisticRegression' in self.models and 'coefficients' in self.models['LogisticRegression']:
            coef_df = self.models['LogisticRegression']['coefficients'].head(10)
            
            colors = ['red' if x < 0 else 'blue' for x in coef_df['coefficient']]
            axes[1].barh(coef_df['feature'], coef_df['coefficient'], color=colors, alpha=0.7)
            axes[1].set_xlabel('ä¿‚æ•°')
            axes[1].set_title('LogisticRegression å›å¸°ä¿‚æ•°')
            axes[1].invert_yaxis()
            axes[1].axvline(x=0, color='black', linestyle='--', alpha=0.5)
        
        plt.tight_layout()
        
        output_path = self.figures_dir / "feature_importance.png"
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ“ ç‰¹å¾´é‡é‡è¦åº¦å›³ä¿å­˜: {output_path}")
    
    def _create_confusion_matrices(self):
        """æ··åŒè¡Œåˆ—ä½œæˆ"""
        n_models = len([m for m in self.models.keys() if 'confusion_matrix' in self.models[m]])
        if n_models == 0:
            return
        
        fig, axes = plt.subplots(1, n_models, figsize=(5*n_models, 4))
        if n_models == 1:
            axes = [axes]
        
        model_idx = 0
        for model_name, model_data in self.models.items():
            if 'confusion_matrix' in model_data:
                cm = model_data['confusion_matrix']
                
                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[model_idx])
                axes[model_idx].set_title(f'{model_name}\næ··åŒè¡Œåˆ—')
                axes[model_idx].set_xlabel('äºˆæ¸¬ãƒ©ãƒ™ãƒ«')
                axes[model_idx].set_ylabel('å®Ÿéš›ã®ãƒ©ãƒ™ãƒ«')
                
                model_idx += 1
        
        plt.tight_layout()
        
        output_path = self.figures_dir / "confusion_matrices.png"
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ“ æ··åŒè¡Œåˆ—ä¿å­˜: {output_path}")
    
    def _create_prediction_distribution_plot(self):
        """äºˆæ¸¬çµæœåˆ†å¸ƒãƒ—ãƒ­ãƒƒãƒˆ"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # å®Ÿéš›ã®åˆ†å¸ƒ
        self.y.value_counts().sort_index().plot(kind='bar', ax=axes[0,0], color='lightblue')
        axes[0,0].set_title('å®Ÿéš›ã®ç†è§£åº¦åˆ†å¸ƒ')
        axes[0,0].set_xlabel('ç†è§£åº¦ãƒ¬ãƒ™ãƒ«')
        axes[0,0].set_ylabel('é »åº¦')
        axes[0,0].tick_params(axis='x', rotation=0)
        
        # å„ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬åˆ†å¸ƒ
        model_idx = 1
        for model_name, model_data in self.models.items():
            if 'predictions' in model_data and model_idx < 4:
                row, col = divmod(model_idx, 2)
                
                pred_series = pd.Series(model_data['predictions'])
                pred_series.value_counts().sort_index().plot(kind='bar', ax=axes[row,col], color='lightcoral')
                axes[row,col].set_title(f'{model_name} äºˆæ¸¬åˆ†å¸ƒ')
                axes[row,col].set_xlabel('ç†è§£åº¦ãƒ¬ãƒ™ãƒ«')
                axes[row,col].set_ylabel('é »åº¦')
                axes[row,col].tick_params(axis='x', rotation=0)
                
                model_idx += 1
        
        # æœªä½¿ç”¨ã®è»¸ã‚’éè¡¨ç¤º
        for i in range(model_idx, 4):
            row, col = divmod(i, 2)
            axes[row,col].axis('off')
        
        plt.tight_layout()
        
        output_path = self.figures_dir / "prediction_distributions.png"
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ“ äºˆæ¸¬åˆ†å¸ƒå›³ä¿å­˜: {output_path}")
    
    def interpret_results(self):
        """çµæœè§£é‡ˆã¨æ•™è‚²çš„ç¤ºå”†"""
        print("\nğŸ“ çµæœè§£é‡ˆä¸­...")
        
        interpretations = {
            'model_performance': self._interpret_model_performance(),
            'feature_insights': self._interpret_feature_importance(),
            'educational_implications': self._generate_educational_implications(),
            'methodological_notes': self._generate_methodological_notes()
        }
        
        self.results['interpretations'] = interpretations
        
    def _interpret_model_performance(self):
        """ãƒ¢ãƒ‡ãƒ«æ€§èƒ½è§£é‡ˆ"""
        best_model = max(self.evaluation_results.items(), key=lambda x: x[1]['cv_f1_mean'])
        best_model_name, best_metrics = best_model
        
        interpretation = {
            'best_model': best_model_name,
            'best_f1_score': best_metrics['cv_f1_mean'],
            'best_accuracy': best_metrics['cv_accuracy_mean'],
            'performance_level': self._classify_performance_level(best_metrics['cv_f1_mean']),
            'model_reliability': self._assess_model_reliability(best_metrics),
            'practical_utility': self._assess_practical_utility(best_metrics)
        }
        
        return interpretation
    
    def _classify_performance_level(self, f1_score):
        """æ€§èƒ½ãƒ¬ãƒ™ãƒ«åˆ†é¡"""
        if f1_score >= 0.8:
            return "å„ªç§€"
        elif f1_score >= 0.7:
            return "è‰¯å¥½"
        elif f1_score >= 0.6:
            return "ä¸­ç¨‹åº¦"
        else:
            return "æ”¹å–„ãŒå¿…è¦"
    
    def _assess_model_reliability(self, metrics):
        """ãƒ¢ãƒ‡ãƒ«ä¿¡é ¼æ€§è©•ä¾¡"""
        cv_std = metrics['cv_f1_std']
        if cv_std <= 0.05:
            return "éå¸¸ã«å®‰å®š"
        elif cv_std <= 0.1:
            return "å®‰å®š"
        else:
            return "ã‚„ã‚„ä¸å®‰å®š"
    
    def _assess_practical_utility(self, metrics):
        """å®Ÿç”¨æ€§è©•ä¾¡"""
        accuracy = metrics['cv_accuracy_mean']
        if accuracy >= 0.8:
            return "æ•™è‚²ç¾å ´ã§ã®å®Ÿç”¨æ€§é«˜"
        elif accuracy >= 0.7:
            return "å‚è€ƒæŒ‡æ¨™ã¨ã—ã¦æœ‰ç”¨"
        else:
            return "è¿½åŠ æ”¹å–„ãŒå¿…è¦"
    
    def _interpret_feature_importance(self):
        """ç‰¹å¾´é‡é‡è¦åº¦è§£é‡ˆ"""
        insights = {
            'most_important_features': [],
            'learning_factors': [],
            'class_effects': []
        }
        
        if 'RandomForest' in self.models and 'feature_importance' in self.models['RandomForest']:
            importance_df = self.models['RandomForest']['feature_importance']
            
            # ä¸Šä½3ã¤ã®é‡è¦ç‰¹å¾´é‡
            top_features = importance_df.head(3)
            for _, row in top_features.iterrows():
                insights['most_important_features'].append({
                    'feature': row['feature'],
                    'importance': row['importance'],
                    'interpretation': self._interpret_feature_meaning(row['feature'])
                })
            
            # å­¦ç¿’è¦å› ã¨èƒŒæ™¯è¦å› ã®åˆ†é›¢
            for _, row in importance_df.iterrows():
                if row['feature'] in ['Q4_ExperimentInterestRating', 'Q5_NewLearningsRating']:
                    insights['learning_factors'].append({
                        'feature': row['feature'],
                        'importance': row['importance']
                    })
                elif 'class_' in row['feature']:
                    insights['class_effects'].append({
                        'feature': row['feature'],
                        'importance': row['importance']
                    })
        
        return insights
    
    def _interpret_feature_meaning(self, feature_name):
        """ç‰¹å¾´é‡ã®æ„å‘³è§£é‡ˆ"""
        interpretations = {
            'Q1_total': 'åŸºç¤çš„ãªæº¶è§£æ¦‚å¿µã®ç†è§£åº¦ï¼ˆæˆæ¥­å‰çŸ¥è­˜ã®å½±éŸ¿ï¼‰',
            'Q3_total': 'ãŠèŒ¶ã«é–¢ã™ã‚‹å…·ä½“çš„çŸ¥è­˜ï¼ˆèº«è¿‘ãªäº‹ä¾‹ã¸ã®é©ç”¨ï¼‰',
            'Q4_ExperimentInterestRating': 'å®Ÿé¨“æ´»å‹•ã¸ã®èˆˆå‘³ãƒ»é–¢å¿ƒåº¦ï¼ˆå­¦ç¿’å‹•æ©Ÿï¼‰',
            'Q5_NewLearningsRating': 'æ–°ã—ã„å­¦ã³ã¸ã®è‡ªè¦šåº¦ï¼ˆãƒ¡ã‚¿èªçŸ¥ï¼‰',
            'class_1.0': 'ã‚¯ãƒ©ã‚¹1ã®ç‰¹å¾´ï¼ˆæŒ‡å°ç’°å¢ƒãƒ»é›†å›£ç‰¹æ€§ï¼‰',
            'class_2.0': 'ã‚¯ãƒ©ã‚¹2ã®ç‰¹å¾´ï¼ˆæŒ‡å°ç’°å¢ƒãƒ»é›†å›£ç‰¹æ€§ï¼‰',
            'class_3.0': 'ã‚¯ãƒ©ã‚¹3ã®ç‰¹å¾´ï¼ˆæŒ‡å°ç’°å¢ƒãƒ»é›†å›£ç‰¹æ€§ï¼‰',
            'class_4.0': 'ã‚¯ãƒ©ã‚¹4ã®ç‰¹å¾´ï¼ˆæŒ‡å°ç’°å¢ƒãƒ»é›†å›£ç‰¹æ€§ï¼‰'
        }
        return interpretations.get(feature_name, 'æœªå®šç¾©ã®ç‰¹å¾´é‡')
    
    def _generate_educational_implications(self):
        """æ•™è‚²çš„ç¤ºå”†ç”Ÿæˆ"""
        implications = {
            'instruction_strategies': [
                "å®Ÿé¨“æ´»å‹•ã¸ã®èˆˆå‘³å–šèµ·ãŒç†è§£åº¦å‘ä¸Šã®éµã¨ãªã‚‹è¦å› ",
                "åŸºç¤æ¦‚å¿µã®ç¢ºå®Ÿãªç†è§£ãŒç™ºå±•çš„å­¦ç¿’ã®åŸºç›¤",
                "æ–°ã—ã„å­¦ã³ã¸ã®è‡ªè¦šã‚’ä¿ƒã™ãƒ¡ã‚¿èªçŸ¥æ”¯æ´ã®é‡è¦æ€§",
                "ã‚¯ãƒ©ã‚¹ç‰¹æ€§ã‚’è€ƒæ…®ã—ãŸå€‹åˆ¥åŒ–æŒ‡å°ã®å¿…è¦æ€§"
            ],
            'assessment_insights': [
                "ç†è§£åº¦äºˆæ¸¬ã«ãŠã‘ã‚‹è¤‡åˆçš„è¦å› ã®é‡è¦æ€§",
                "å­¦ç¿’å‹•æ©Ÿã¨èªçŸ¥è¦å› ã®ç›¸äº’ä½œç”¨",
                "é‡çš„æŒ‡æ¨™ã«ã‚ˆã‚‹å­¦ç¿’æˆæœã®å®¢è¦³çš„è©•ä¾¡ã®å¯èƒ½æ€§"
            ],
            'curriculum_design': [
                "æ®µéšçš„ãªæ¦‚å¿µå½¢æˆã‚’æ”¯æ´ã™ã‚‹æ•™æé–‹ç™º",
                "å®Ÿé¨“ã¨ç†è«–ã®åŠ¹æœçš„ãªçµ±åˆæ–¹æ³•",
                "å€‹äººå·®ã«å¯¾å¿œã—ãŸå¤šæ§˜ãªå­¦ç¿’æ”¯æ´ç­–"
            ]
        }
        
        return implications
    
    def _generate_methodological_notes(self):
        """æ–¹æ³•è«–çš„æ³¨æ„äº‹é …"""
        notes = {
            'sample_size_limitations': [
                f"é™å®šçš„ãªã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºï¼ˆN={len(self.y)}ï¼‰ã«ã‚ˆã‚‹äºˆæ¸¬ç²¾åº¦ã®åˆ¶ç´„",
                "äº¤å·®æ¤œè¨¼ã«ã‚ˆã‚‹æ±åŒ–æ€§èƒ½ã®è©•ä¾¡ã®é‡è¦æ€§",
                "è¿½åŠ ãƒ‡ãƒ¼ã‚¿åé›†ã«ã‚ˆã‚‹äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«æ”¹å–„ã®å¿…è¦æ€§"
            ],
            'feature_engineering': [
                "ç¾åœ¨ã®ç‰¹å¾´é‡è¨­è¨ˆã®å¦¥å½“æ€§ã¨æ”¹å–„å¯èƒ½æ€§",
                "ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç­‰ã®è³ªçš„æƒ…å ±ã®æ´»ç”¨æ¤œè¨",
                "æ™‚ç³»åˆ—è¦ç´ ï¼ˆæˆæ¥­å‰å¾Œå¤‰åŒ–ï¼‰ã®å°†æ¥çš„ãªçµ„ã¿è¾¼ã¿"
            ],
            'model_interpretability': [
                "æ•™è‚²ç¾å ´ã§ã®è§£é‡ˆå¯èƒ½æ€§ã‚’é‡è¦–ã—ãŸãƒ¢ãƒ‡ãƒ«é¸æŠ",
                "ç‰¹å¾´é‡é‡è¦åº¦ã®æ•™è‚²å­¦çš„æ„å‘³ã®ç¶™ç¶šçš„æ¤œè¨¼",
                "äºˆæ¸¬çµæœã®æ•™è‚²å®Ÿè·µã¸ã®é©ç”¨ã«ãŠã‘ã‚‹æ³¨æ„ç‚¹"
            ]
        }
        
        return notes
    
    def save_results(self):
        """çµæœä¿å­˜"""
        print("\nğŸ’¾ çµæœä¿å­˜ä¸­...")
        
        # è©³ç´°çµæœæº–å‚™
        detailed_results = {
            'metadata': {
                'analysis_type': 'Machine Learning Prediction',
                'generated_at': datetime.now().isoformat(),
                'sample_size': len(self.y),
                'features_used': self.feature_names,
                'target_variable': 'Q6_DissolvingUnderstandingRating'
            },
            'model_performance': self.evaluation_results,
            'feature_analysis': {},
            'interpretations': self.results.get('interpretations', {}),
            'models_summary': {}
        }
        
        # ç‰¹å¾´é‡é‡è¦åº¦æƒ…å ±è¿½åŠ 
        for model_name, model_data in self.models.items():
            if 'feature_importance' in model_data:
                detailed_results['feature_analysis'][model_name] = model_data['feature_importance'].to_dict('records')
            if 'coefficients' in model_data:
                detailed_results['feature_analysis'][model_name + '_coefficients'] = model_data['coefficients'].to_dict('records')
        
        # ãƒ¢ãƒ‡ãƒ«è¦ç´„æƒ…å ±
        for model_name, model_data in self.models.items():
            if 'cv_accuracy' in model_data:
                detailed_results['models_summary'][model_name] = {
                    'cross_validation_accuracy': model_data['cv_accuracy'].tolist(),
                    'cross_validation_f1': model_data['cv_f1_weighted'].tolist(),
                    'final_metrics': {
                        'accuracy': model_data['final_accuracy'],
                        'f1_weighted': model_data['final_f1_weighted']
                    }
                }
        
        # JSONä¿å­˜
        output_path = self.output_dir / "machine_learning_prediction_results.json"
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(detailed_results, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"âœ“ è©³ç´°çµæœä¿å­˜: {output_path}")
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼ˆæœ€è‰¯ãƒ¢ãƒ‡ãƒ«ï¼‰
        self._save_best_model()
        
        # è¦ç´„ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
        self._create_summary_report(detailed_results)
    
    def _save_best_model(self):
        """æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ä¿å­˜"""
        if self.evaluation_results:
            best_model_name = max(self.evaluation_results.items(), key=lambda x: x[1]['cv_f1_mean'])[0]
            best_model_data = self.models[best_model_name]
            
            model_package = {
                'model': best_model_data['model'],
                'scaler': best_model_data.get('scaler'),
                'feature_names': self.feature_names,
                'model_name': best_model_name,
                'performance_metrics': self.evaluation_results[best_model_name]
            }
            
            model_path = self.output_dir / "best_prediction_model.pkl"
            with open(model_path, 'wb') as f:
                pickle.dump(model_package, f)
            
            print(f"âœ“ æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {model_path}")
    
    def _create_summary_report(self, detailed_results):
        """è¦ç´„ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ"""
        report_lines = [
            "# æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹ç†è§£åº¦äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«åˆ†æãƒ¬ãƒãƒ¼ãƒˆ",
            "## å°å­¦æ ¡å‡ºå‰æˆæ¥­ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆ - äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰",
            "",
            f"**ç”Ÿæˆæ—¥æ™‚**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"**ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º**: {detailed_results['metadata']['sample_size']} ä»¶",
            f"**ç›®çš„å¤‰æ•°**: {detailed_results['metadata']['target_variable']} (ç†è§£åº¦4æ®µéš)",
            "",
            "## åˆ†ææ¦‚è¦",
            "",
            "æˆæ¥­å¾Œãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ãŸç†è§£åº¦äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã€å­¦ç¿’æˆæœã«å½±éŸ¿ã™ã‚‹è¦å› ã‚’ç‰¹å®šã€‚",
            "è¤‡æ•°ã®æ©Ÿæ¢°å­¦ç¿’æ‰‹æ³•ã‚’æ¯”è¼ƒã—ã€æ•™è‚²ç¾å ´ã§ã®å®Ÿç”¨æ€§ã‚’è©•ä¾¡ã€‚",
            "",
            "## ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒ",
            ""
        ]
        
        # ãƒ¢ãƒ‡ãƒ«æ€§èƒ½è¡¨
        report_lines.append("| ãƒ¢ãƒ‡ãƒ« | CVç²¾åº¦ | CV F1ã‚¹ã‚³ã‚¢ | æœ€çµ‚ç²¾åº¦ |")
        report_lines.append("|--------|--------|-------------|----------|")
        
        for model_name, metrics in detailed_results['model_performance'].items():
            report_lines.append(
                f"| {model_name} | {metrics['cv_accuracy_mean']:.3f}Â±{metrics['cv_accuracy_std']:.3f} | "
                f"{metrics['cv_f1_mean']:.3f}Â±{metrics['cv_f1_std']:.3f} | {metrics['final_accuracy']:.3f} |"
            )
        
        # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«
        best_model = max(detailed_results['model_performance'].items(), key=lambda x: x[1]['cv_f1_mean'])
        report_lines.extend([
            "",
            f"**æœ€è‰¯ãƒ¢ãƒ‡ãƒ«**: {best_model[0]}",
            f"**CV F1ã‚¹ã‚³ã‚¢**: {best_model[1]['cv_f1_mean']:.3f}",
            ""
        ])
        
        # ç‰¹å¾´é‡é‡è¦åº¦
        if 'feature_analysis' in detailed_results:
            report_lines.extend([
                "## é‡è¦ãªå­¦ç¿’è¦å› ",
                ""
            ])
            
            # RandomForestã®é‡è¦åº¦ãŒã‚ã‚‹å ´åˆ
            rf_importance = detailed_results['feature_analysis'].get('RandomForest')
            if rf_importance:
                report_lines.append("### ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆRandomForestï¼‰")
                report_lines.append("")
                for feature_data in rf_importance[:5]:
                    report_lines.append(f"- **{feature_data['feature']}**: {feature_data['importance']:.3f}")
                report_lines.append("")
        
        # æ•™è‚²çš„ç¤ºå”†
        if 'interpretations' in detailed_results and 'educational_implications' in detailed_results['interpretations']:
            implications = detailed_results['interpretations']['educational_implications']
            
            report_lines.extend([
                "## æ•™è‚²çš„ç¤ºå”†",
                "",
                "### æŒ‡å°æˆ¦ç•¥ã¸ã®æè¨€",
                ""
            ])
            
            for strategy in implications['instruction_strategies']:
                report_lines.append(f"- {strategy}")
            
            report_lines.extend([
                "",
                "### è©•ä¾¡ãƒ»ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ è¨­è¨ˆã¸ã®ç¤ºå”†",
                ""
            ])
            
            for insight in implications['assessment_insights']:
                report_lines.append(f"- {insight}")
        
        # åˆ¶ç´„ã¨é™ç•Œ
        report_lines.extend([
            "",
            "## åˆ†æã®åˆ¶ç´„ã¨é™ç•Œ",
            "",
            f"- é™å®šçš„ãªã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºï¼ˆN={detailed_results['metadata']['sample_size']}ï¼‰",
            "- ç‹¬ç«‹ç¾¤æ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ã®ãŸã‚å€‹äººã®å¤‰åŒ–ã¯è€ƒæ…®ä¸å¯",
            "- è¿½åŠ ç‰¹å¾´é‡ï¼ˆãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç­‰ï¼‰ã®æ´»ç”¨ä½™åœ°",
            "- æ•™è‚²ç¾å ´ã§ã®å®Ÿè£…ã«ãŠã‘ã‚‹è§£é‡ˆå¯èƒ½æ€§ã®é‡è¦æ€§",
            "",
            "## ä»Šå¾Œã®æ”¹å–„ææ¡ˆ",
            "",
            "1. **ãƒ‡ãƒ¼ã‚¿æ‹¡å……**: ã‚ˆã‚Šå¤§è¦æ¨¡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®æ¤œè¨¼",
            "2. **ç‰¹å¾´é‡æ‹¡å¼µ**: ãƒ†ã‚­ã‚¹ãƒˆåˆ†æçµæœã®çµ±åˆ",
            "3. **ç¸¦æ–­åˆ†æ**: å€‹äººè¿½è·¡å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã§ã®å­¦ç¿’éç¨‹åˆ†æ",
            "4. **å®Ÿç”¨åŒ–æ¤œè¨**: æ•™è‚²ç¾å ´ã§ã®äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ å°å…¥",
            "",
            "---",
            "",
            "**Generated by**: Claude Code Analysis (ML Prediction Implementation)",
            f"**Model Files**: {self.output_dir}",
            f"**Visualizations**: {self.figures_dir}"
        ])
        
        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report_path = self.output_dir / "ml_prediction_summary.txt"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        
        print(f"âœ“ è¦ç´„ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")
    
    def run_complete_analysis(self):
        """å®Œå…¨æ©Ÿæ¢°å­¦ç¿’åˆ†æå®Ÿè¡Œ"""
        print("="*60)
        print("æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹ç†è§£åº¦äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰")
        print("="*60)
        print(f"é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print()
        
        try:
            # åˆ†æå®Ÿè¡Œ
            self.load_data()
            self.prepare_ml_data()
            self.train_models()
            self.hyperparameter_tuning()
            self.evaluate_models()
            self.create_visualizations()
            self.interpret_results()
            self.save_results()
            
            print("\n" + "="*60)
            print("ğŸ‰ æ©Ÿæ¢°å­¦ç¿’åˆ†æå®Œäº†!")
            print("="*60)
            print(f"çµ‚äº†æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print()
            print("ğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«:")
            print(f"  - è©³ç´°çµæœ: {self.output_dir}/machine_learning_prediction_results.json")
            print(f"  - è¦ç´„ãƒ¬ãƒãƒ¼ãƒˆ: {self.output_dir}/ml_prediction_summary.txt")
            print(f"  - æœ€è‰¯ãƒ¢ãƒ‡ãƒ«: {self.output_dir}/best_prediction_model.pkl")
            print(f"  - å›³è¡¨: {self.figures_dir}/")
            print()
            
            # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«æƒ…å ±è¡¨ç¤º
            if hasattr(self, 'evaluation_results') and self.evaluation_results:
                best_model = max(self.evaluation_results.items(), key=lambda x: x[1]['cv_f1_mean'])
                print(f"ğŸ† æœ€è‰¯ãƒ¢ãƒ‡ãƒ«: {best_model[0]}")
                print(f"   CV F1ã‚¹ã‚³ã‚¢: {best_model[1]['cv_f1_mean']:.3f}")
                print(f"   CVç²¾åº¦: {best_model[1]['cv_accuracy_mean']:.3f}")
            
            return True
            
        except Exception as e:
            print(f"\nâŒ æ©Ÿæ¢°å­¦ç¿’åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            print(f"è©³ç´°: {traceback.format_exc()}")
            return False

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    project_root = Path(__file__).parent.parent.parent
    
    predictor = MachineLearningPredictor(project_root)
    
    try:
        success = predictor.run_complete_analysis()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\nğŸ›‘ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()