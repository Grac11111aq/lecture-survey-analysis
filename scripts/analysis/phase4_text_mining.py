#!/usr/bin/env python3
"""
Phase 4: ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°
æ„Ÿæƒ³æ–‡ã®åˆ†æã€é »å‡ºèªã€æ„Ÿæƒ…åˆ†æ
"""

import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from collections import Counter
import re
import os

def load_data():
    """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
    data_dir = 'data/analysis/'
    
    # æ„Ÿæƒ³ãƒ‡ãƒ¼ã‚¿
    comment_df = pd.read_csv(data_dir + 'comment.csv')
    
    # æˆæ¥­å¾Œãƒ‡ãƒ¼ã‚¿ï¼ˆç†ç”±ç­‰ã®è‡ªç”±è¨˜è¿°ï¼‰
    after_df = pd.read_csv(data_dir + 'after_excel_compliant.csv')
    
    print(f"æ„Ÿæƒ³ãƒ‡ãƒ¼ã‚¿: {comment_df.shape}")
    print(f"æˆæ¥­å¾Œãƒ‡ãƒ¼ã‚¿: {after_df.shape}")
    
    return comment_df, after_df

def preprocess_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†"""
    if pd.isna(text) or text == '':
        return ''
    
    # æ–‡å­—åˆ—ã«å¤‰æ›
    text = str(text)
    
    # åŸºæœ¬çš„ãªã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
    text = re.sub(r'[ã€‚ã€ï¼ï¼Ÿ\n\r\t]', ' ', text)  # å¥èª­ç‚¹ã‚’ç©ºç™½ã«
    text = re.sub(r'\s+', ' ', text)  # è¤‡æ•°ç©ºç™½ã‚’å˜ä¸€ç©ºç™½ã«
    text = text.strip()
    
    return text

def extract_keywords(texts, min_length=2):
    """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
    
    # ç†ç§‘ãƒ»åŒ–å­¦é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    science_keywords = [
        'å®Ÿé¨“', 'çµæ™¶', 'ç‚', 'è‰²', 'æ°´æº¶æ¶²', 'æº¶ã‘ã‚‹', 'å¡©', 'ç ‚ç³–',
        'é¢ç™½', 'ãŠã‚‚ã—ã‚', 'æ¥½ã—ã„', 'ãŸã®ã—', 'ã™ã”ã„', 'ãã‚Œã„', 'ç¾ã—ã„',
        'é©šã', 'ã³ã£ãã‚Š', 'ä¸æ€è­°', 'ãµã—ã', 'ç™ºè¦‹', 'ã‚ã‹ã£ãŸ', 'ç†è§£',
        'å­¦ã‚“ã ', 'è¦šãˆãŸ', 'çŸ¥ã£ãŸ', 'ã¿ã', 'é†¤æ²¹', 'æ³¥æ°´', 'å¢¨æ±',
        'å†çµæ™¶', 'ç‚è‰²åå¿œ', 'ãƒŠãƒˆãƒªã‚¦ãƒ ', 'é£Ÿå¡©', 'ç†ç§‘', 'ç§‘å­¦',
        'å…ˆç”Ÿ', 'æˆæ¥­', 'å‹‰å¼·', 'å­¦ç¿’', 'ä½“é¨“', 'è¦³å¯Ÿ', 'ã‚„ã£ã¦ã¿ãŸ'
    ]
    
    # å…¨ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
    all_text = ' '.join([preprocess_text(text) for text in texts if pd.notna(text)])
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚«ã‚¦ãƒ³ãƒˆ
    keyword_counts = {}
    for keyword in science_keywords:
        count = all_text.count(keyword)
        if count > 0:
            keyword_counts[keyword] = count
    
    # ä¸€èˆ¬çš„ãªèªå¥ã‚‚æŠ½å‡ºï¼ˆã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠãƒ»æ¼¢å­—ã®2æ–‡å­—ä»¥ä¸Šï¼‰
    words = re.findall(r'[ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠæ¼¢å­—]{2,}', all_text)
    general_counts = Counter([w for w in words if len(w) >= min_length])
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆã‚ã¾ã‚Šæ„å‘³ã®ãªã„èªã‚’é™¤å¤–ï¼‰
    exclude_words = ['ã§ã™', 'ã¾ã™', 'ã—ãŸ', 'ã‚ã‚‹', 'ã„ã‚‹', 'ãªã‚‹', 'ã™ã‚‹', 'ã¨ã„ã†', 'ã®ã§', 'ã‹ã‚‰', 'ãŸã‚']
    general_counts = {k: v for k, v in general_counts.items() if k not in exclude_words}
    
    return keyword_counts, general_counts

def sentiment_analysis(texts):
    """æ„Ÿæƒ…åˆ†æï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
    
    positive_words = [
        'é¢ç™½', 'ãŠã‚‚ã—ã‚', 'æ¥½ã—ã„', 'ãŸã®ã—', 'ã™ã”ã„', 'ã‚ˆã‹ã£ãŸ', 
        'ãã‚Œã„', 'ç¾ã—ã„', 'ç´ æ™´ã‚‰ã—ã„', 'æ„Ÿå‹•', 'å¥½ã', 'ã¨ã¦ã‚‚',
        'ã³ã£ãã‚Š', 'é©šã', 'ä¸æ€è­°', 'ãµã—ã', 'ã‚ã‹ã£ãŸ', 'ç†è§£',
        'ã‚ã‚ŠãŒã¨ã†', 'æ„Ÿè¬', 'ã‚„ã£ã¦ã¿ãŸã„', 'ã‚‚ã£ã¨', 'ã¾ãŸ'
    ]
    
    negative_words = [
        'é›£ã—ã„', 'ã‚€ãšã‹ã—', 'ã‚ã‹ã‚‰ãªã„', 'ã¤ã¾ã‚‰ãªã„', 'å«Œ',
        'å›°ã£ãŸ', 'å•é¡Œ', 'å¤±æ•—', 'ã ã‚', 'ã„ã‘ãªã„'
    ]
    
    neutral_words = [
        'æ™®é€š', 'ã¾ã‚ã¾ã‚', 'ãã“ãã“', 'ä¸€èˆ¬çš„', 'æ™®æ®µ'
    ]
    
    results = []
    
    for text in texts:
        text_clean = preprocess_text(text)
        
        pos_count = sum(1 for word in positive_words if word in text_clean)
        neg_count = sum(1 for word in negative_words if word in text_clean)
        neu_count = sum(1 for word in neutral_words if word in text_clean)
        
        # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ç®—å‡º
        total_count = pos_count + neg_count + neu_count
        if total_count > 0:
            sentiment_score = (pos_count - neg_count) / total_count
            if sentiment_score > 0.2:
                sentiment = 'positive'
            elif sentiment_score < -0.2:
                sentiment = 'negative'
            else:
                sentiment = 'neutral'
        else:
            sentiment = 'neutral'
            sentiment_score = 0
        
        results.append({
            'text': text,
            'positive_count': pos_count,
            'negative_count': neg_count,
            'neutral_count': neu_count,
            'sentiment_score': sentiment_score,
            'sentiment': sentiment
        })
    
    return results

def analyze_comments(comment_df):
    """æ„Ÿæƒ³æ–‡åˆ†æ"""
    print("\n=== æ„Ÿæƒ³æ–‡åˆ†æ ===")
    
    # æ„Ÿæƒ³æ–‡ã®æŠ½å‡º
    comments = comment_df['comment'].dropna().tolist()
    
    print(f"åˆ†æå¯¾è±¡æ„Ÿæƒ³æ–‡: {len(comments)}ä»¶")
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
    keyword_counts, general_counts = extract_keywords(comments)
    
    print(f"\nç†ç§‘é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ (ä¸Šä½10å€‹):")
    sorted_keywords = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)[:10]
    for word, count in sorted_keywords:
        print(f"  {word}: {count}å›")
    
    print(f"\nä¸€èˆ¬èªå¥ (ä¸Šä½15å€‹):")
    sorted_general = sorted(general_counts.items(), key=lambda x: x[1], reverse=True)[:15]
    for word, count in sorted_general:
        print(f"  {word}: {count}å›")
    
    # æ„Ÿæƒ…åˆ†æ
    sentiment_results = sentiment_analysis(comments)
    
    sentiment_summary = pd.DataFrame(sentiment_results)['sentiment'].value_counts()
    print(f"\næ„Ÿæƒ…åˆ†æçµæœ:")
    for sentiment, count in sentiment_summary.items():
        pct = count / len(comments) * 100
        print(f"  {sentiment}: {count}ä»¶ ({pct:.1f}%)")
    
    # ä»£è¡¨çš„ãªæ„Ÿæƒ³ã®æŠ½å‡º
    print(f"\nä»£è¡¨çš„ãªæ„Ÿæƒ³:")
    
    # ãƒã‚¸ãƒ†ã‚£ãƒ–ãªæ„Ÿæƒ³
    positive_comments = [r for r in sentiment_results if r['sentiment'] == 'positive']
    if positive_comments:
        # æœ€ã‚‚ãƒã‚¸ãƒ†ã‚£ãƒ–ãªæ„Ÿæƒ³
        most_positive = max(positive_comments, key=lambda x: x['sentiment_score'])
        print(f"æœ€ã‚‚ãƒã‚¸ãƒ†ã‚£ãƒ–: {most_positive['text'][:100]}...")
    
    # ãƒã‚¬ãƒ†ã‚£ãƒ–ãªæ„Ÿæƒ³ï¼ˆã‚ã‚Œã°ï¼‰
    negative_comments = [r for r in sentiment_results if r['sentiment'] == 'negative']
    if negative_comments:
        most_negative = max(negative_comments, key=lambda x: abs(x['sentiment_score']))
        print(f"ãƒã‚¬ãƒ†ã‚£ãƒ–: {most_negative['text'][:100]}...")
    
    return {
        'keyword_counts': keyword_counts,
        'general_counts': general_counts,
        'sentiment_results': sentiment_results,
        'sentiment_summary': sentiment_summary
    }

def analyze_structured_text(after_df):
    """æ§‹é€ åŒ–ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†æ"""
    print("\n=== æ§‹é€ åŒ–ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ ===")
    
    # Q2: å‘³å™Œæ±ãŒã—ã‚‡ã£ã±ã„ç†ç”±
    if 'Q2_MisoSaltyReason' in after_df.columns:
        miso_reasons = after_df['Q2_MisoSaltyReason'].dropna().tolist()
        print(f"\nQ2 å‘³å™Œæ±ã®ç†ç”± ({len(miso_reasons)}ä»¶):")
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ
        keyword_counts, _ = extract_keywords(miso_reasons)
        
        # ç§‘å­¦çš„ç”¨èªã®ä½¿ç”¨çŠ¶æ³
        scientific_terms = ['ãƒŠãƒˆãƒªã‚¦ãƒ ', 'å¡©', 'é£Ÿå¡©', 'æˆåˆ†', 'æº¶ã‘ã‚‹', 'å«ã¾ã‚Œ']
        science_usage = {}
        
        for term in scientific_terms:
            count = sum(1 for reason in miso_reasons if term in str(reason))
            if count > 0:
                science_usage[term] = count
                pct = count / len(miso_reasons) * 100
                print(f"  '{term}' ä½¿ç”¨: {count}ä»¶ ({pct:.1f}%)")
        
        # ä»£è¡¨çš„ãªå›ç­”
        print(f"\nä»£è¡¨çš„ãªå›ç­”ä¾‹:")
        for i, reason in enumerate(miso_reasons[:5]):
            print(f"  {i+1}. {reason}")
        
        return science_usage
    
    return {}

def compare_text_by_performance(comment_df, after_df):
    """æˆç¸¾åˆ¥ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ"""
    print("\n=== æˆç¸¾åˆ¥ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ ===")
    
    if 'Q6_DissolvingUnderstandingRating' in after_df.columns:
        # ç†è§£åº¦åˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        understanding_groups = {
            'high': after_df[after_df['Q6_DissolvingUnderstandingRating'] == 4],
            'medium': after_df[after_df['Q6_DissolvingUnderstandingRating'].isin([2, 3])],
            'low': after_df[after_df['Q6_DissolvingUnderstandingRating'] == 1]
        }
        
        for level, group_df in understanding_groups.items():
            if len(group_df) > 0:
                print(f"\n{level.upper()}ç¾¤ (N={len(group_df)}):")
                
                # Q2ã®å›ç­”åˆ†æ
                if 'Q2_MisoSaltyReason' in group_df.columns:
                    reasons = group_df['Q2_MisoSaltyReason'].dropna().tolist()
                    if reasons:
                        keyword_counts, _ = extract_keywords(reasons)
                        
                        # ç§‘å­¦ç”¨èªä½¿ç”¨ç‡
                        science_terms = ['ãƒŠãƒˆãƒªã‚¦ãƒ ', 'å¡©', 'æˆåˆ†']
                        science_rate = 0
                        for term in science_terms:
                            science_rate += sum(1 for r in reasons if term in str(r))
                        
                        science_rate = science_rate / len(reasons) * 100 if reasons else 0
                        print(f"  ç§‘å­¦ç”¨èªä½¿ç”¨ç‡: {science_rate:.1f}%")

def create_visualizations(analysis_results):
    """å¯è¦–åŒ–ä½œæˆ"""
    print("\n=== ãƒ†ã‚­ã‚¹ãƒˆåˆ†æå¯è¦–åŒ– ===")
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # 1. é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    if analysis_results['keyword_counts']:
        words = list(analysis_results['keyword_counts'].keys())[:10]
        counts = list(analysis_results['keyword_counts'].values())[:10]
        
        axes[0,0].barh(words, counts)
        axes[0,0].set_title('Top Science Keywords')
        axes[0,0].set_xlabel('Frequency')
    
    # 2. æ„Ÿæƒ…åˆ†æçµæœ
    if not analysis_results['sentiment_summary'].empty:
        sentiment_labels = analysis_results['sentiment_summary'].index
        sentiment_counts = analysis_results['sentiment_summary'].values
        colors = ['green' if s == 'positive' else 'red' if s == 'negative' else 'gray' for s in sentiment_labels]
        
        axes[0,1].pie(sentiment_counts, labels=sentiment_labels, colors=colors, autopct='%1.1f%%')
        axes[0,1].set_title('Sentiment Distribution')
    
    # 3. ä¸€èˆ¬èªå¥ã®é »åº¦
    if analysis_results['general_counts']:
        general_words = list(analysis_results['general_counts'].keys())[:10]
        general_counts = list(analysis_results['general_counts'].values())[:10]
        
        axes[1,0].barh(general_words, general_counts)
        axes[1,0].set_title('Top General Words')
        axes[1,0].set_xlabel('Frequency')
    
    # 4. æ„Ÿæƒ…ã‚¹ã‚³ã‚¢åˆ†å¸ƒ
    sentiment_scores = [r['sentiment_score'] for r in analysis_results['sentiment_results']]
    
    axes[1,1].hist(sentiment_scores, bins=20, alpha=0.7, edgecolor='black')
    axes[1,1].axvline(x=0, color='red', linestyle='--', alpha=0.7)
    axes[1,1].set_title('Sentiment Score Distribution')
    axes[1,1].set_xlabel('Sentiment Score')
    axes[1,1].set_ylabel('Frequency')
    
    plt.tight_layout()
    
    # ä¿å­˜
    output_dir = 'reports/2025-05-30/'
    os.makedirs(output_dir, exist_ok=True)
    plt.savefig(f'{output_dir}phase4_text_results.png', dpi=300, bbox_inches='tight')
    print(f"å›³è¡¨ä¿å­˜: {output_dir}phase4_text_results.png")
    plt.close()

def generate_summary(analysis_results, science_usage):
    """Phase 4 çµæœã‚µãƒãƒªãƒ¼"""
    print("\n" + "="*60)
    print("Phase 4 ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚° çµæœã‚µãƒãƒªãƒ¼")
    print("="*60)
    
    print(f"\nğŸ“ æ„Ÿæƒ³æ–‡åˆ†æçµæœ:")
    
    # æ„Ÿæƒ…åˆ†æçµæœ
    if not analysis_results['sentiment_summary'].empty:
        total_comments = analysis_results['sentiment_summary'].sum()
        positive_count = analysis_results['sentiment_summary'].get('positive', 0)
        positive_rate = positive_count / total_comments * 100
        
        print(f"ãƒ»ç·æ„Ÿæƒ³æ–‡æ•°: {total_comments}ä»¶")
        print(f"ãƒ»ãƒã‚¸ãƒ†ã‚£ãƒ–ç‡: {positive_rate:.1f}% ({positive_count}ä»¶)")
        
        if positive_rate > 70:
            print("ğŸŸ¢ éå¸¸ã«é«˜ã„æº€è¶³åº¦")
        elif positive_rate > 50:
            print("ğŸŸ¡ é«˜ã„æº€è¶³åº¦")
        else:
            print("ğŸŸ  æº€è¶³åº¦è¦æ”¹å–„")
    
    # é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    if analysis_results['keyword_counts']:
        top_keywords = sorted(analysis_results['keyword_counts'].items(), key=lambda x: x[1], reverse=True)[:5]
        print(f"\nğŸ”¤ é »å‡ºç†ç§‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:")
        for word, count in top_keywords:
            print(f"ãƒ»{word}: {count}å›")
    
    # ç§‘å­¦ç”¨èªã®ä½¿ç”¨
    if science_usage:
        print(f"\nğŸ§ª ç§‘å­¦ç”¨èªä½¿ç”¨çŠ¶æ³:")
        total_responses = sum(science_usage.values())
        for term, count in sorted(science_usage.items(), key=lambda x: x[1], reverse=True):
            print(f"ãƒ»{term}: {count}å›ä½¿ç”¨")
        
        # æœ€ã‚‚ä½¿ç”¨ã•ã‚ŒãŸç§‘å­¦ç”¨èª
        most_used = max(science_usage.items(), key=lambda x: x[1])
        print(f"ãƒ»æœ€é »å‡ºç§‘å­¦ç”¨èª: '{most_used[0]}' ({most_used[1]}å›)")
    
    print(f"\nğŸ¯ ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã‹ã‚‰ã®çŸ¥è¦‹:")
    
    # å®Ÿé¨“ã¸ã®åå¿œ
    experiment_words = ['å®Ÿé¨“', 'ç‚', 'è‰²', 'çµæ™¶', 'å†çµæ™¶']
    experiment_mentions = sum(analysis_results['keyword_counts'].get(word, 0) for word in experiment_words)
    
    if experiment_mentions > 0:
        print(f"ãƒ»å®Ÿé¨“é–¢é€£è¨€åŠ: {experiment_mentions}å›ï¼ˆé«˜ã„é–¢å¿ƒï¼‰")
        
    # æ„Ÿæƒ…è¡¨ç¾
    emotion_words = ['é¢ç™½', 'ãŠã‚‚ã—ã‚', 'ã™ã”ã„', 'ãã‚Œã„', 'ä¸æ€è­°']
    emotion_mentions = sum(analysis_results['keyword_counts'].get(word, 0) for word in emotion_words)
    
    if emotion_mentions > 0:
        print(f"ãƒ»æ„Ÿæƒ…è¡¨ç¾: {emotion_mentions}å›ï¼ˆè±Šã‹ãªæ„Ÿæƒ…ä½“é¨“ï¼‰")
    
    # ç†è§£ãƒ»å­¦ç¿’é–¢é€£
    learning_words = ['ã‚ã‹ã£ãŸ', 'ç†è§£', 'å­¦ã‚“ã ', 'è¦šãˆãŸ', 'çŸ¥ã£ãŸ']
    learning_mentions = sum(analysis_results['keyword_counts'].get(word, 0) for word in learning_words)
    
    if learning_mentions > 0:
        print(f"ãƒ»å­¦ç¿’å®Ÿæ„Ÿ: {learning_mentions}å›ï¼ˆå­¦ç¿’åŠ¹æœã®å®Ÿæ„Ÿï¼‰")
    
    print(f"\nâœ… å…¨åˆ†æå®Œäº†:")
    print(f"ãƒ»Phase 1: ãƒ‡ãƒ¼ã‚¿å“è³ªç¢ºèª âœ…")
    print(f"ãƒ»Phase 2: çµ±è¨ˆçš„æ¤œè¨¼ âœ…")  
    print(f"ãƒ»Phase 3: é›†å›£é–“å·®ç•°åˆ†æ âœ…")
    print(f"ãƒ»Phase 4: ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚° âœ…")
    
    print(f"\nğŸ‰ ç·åˆçš„ãªæ•™è‚²åŠ¹æœ:")
    print(f"ãƒ»å®šé‡çš„åŠ¹æœ: éæ°´æº¶æ¶²ç†è§£ã®å¤§å¹…æ”¹å–„")
    print(f"ãƒ»è³ªçš„åŠ¹æœ: é«˜ã„æº€è¶³åº¦ã¨è±Šã‹ãªæ„Ÿæƒ…ä½“é¨“")
    print(f"ãƒ»å€‹åˆ¥åŠ¹æœ: ç†è§£åº¦ã®ä½ã„ç”Ÿå¾’ã«ã‚ˆã‚Šå¤§ããªæ©æµ")
    print(f"ãƒ»ç§‘å­¦çš„æ€è€ƒ: ç§‘å­¦ç”¨èªã®é©åˆ‡ãªä½¿ç”¨å¢—åŠ ")
    
    print("="*60)

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("Phase 4: ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚° å®Ÿè¡Œé–‹å§‹")
    print("="*60)
    
    # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    comment_df, after_df = load_data()
    
    # 2. æ„Ÿæƒ³æ–‡åˆ†æ
    analysis_results = analyze_comments(comment_df)
    
    # 3. æ§‹é€ åŒ–ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ
    science_usage = analyze_structured_text(after_df)
    
    # 4. æˆç¸¾åˆ¥ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ
    compare_text_by_performance(comment_df, after_df)
    
    # 5. å¯è¦–åŒ–
    create_visualizations(analysis_results)
    
    # 6. ã‚µãƒãƒªãƒ¼
    generate_summary(analysis_results, science_usage)
    
    print("\nğŸ‰ Phase 4 ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°å®Œäº†!")
    print("ğŸŠ å…¨åˆ†æãƒ•ã‚§ãƒ¼ã‚ºå®Œäº†!")
    
    return analysis_results, science_usage

if __name__ == "__main__":
    analysis_results, science_usage = main()